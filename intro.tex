%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

A world where cloud compute is run in the format of serverless functions is
attractive to developers and providers: developers pay only for what they use,
while having access to many resources when needed; and cloud providers have
control over scheduling and can use that to drive up utilization, rather than
needing to hold idle resources available for clients who reserved them.


There remain roadblocks, however, that make serverless today infeasible for
workloads that are a good fit. For instance web server applications, which often
have inconsistent and bursty load, but are rarely run completely in serverless
offerings~\cite{reddit-serverless1}, such as AWS lambda. One of the challenges
is lambda invocations' variable end to end latencies: in a small benchmark
(described in Section~\ref{motivation}) we found that total execution time
latencies for a simple hello world function that sleeps for 20 ms ranged from 20
to 400ms; whereas an acceptable latency for user-facing pages is be anything
below 100ms~\cite{page-load-time}. This variability is a problem because it has
been shown that small response time differences can matter a lot in interactive
applications~\cite{amz-page-load,google-page-load}.


A well-known cause of these variable latencies is cold starts. This paper takes
the position that systems research is well underway to reaching low single digit
ms cold start times, with current state-of-the-art research systems pushing into
single digit territory~\cite{sigmaos,mitosis}. Which begs the question: if cold
start is fast enought that more latency sensitive applications, like web
applications, can have a cold start on the critical path, are we then done? Will
serverless then be, at least from an infrastructure perspective, ready to
support these sorts of workloads?

This paper argues that no, there still remains a challenge to running such a
latency sensitive workload on serverless: queueing and delay within the system.
Load will not always fit in the resources providers have, and so some work msut
be queued or otherwise degraded. In the world of long running servers,
developers avoid degradation of access to resources by giving latency critical
services reservations; but reserving servers is incompatible with the serverless
approach. 


What developers care about in the end is that the functions that are latency
sensitive run quickly. This paper assumes a world where cold starts are fast and
latency sensitive work runs alongside map reduce and image processing functions.
The challenge this paper addresses is that latency sensitive functions might end
up behind background ones, waiting to be placed on machines or to get access to
resources. To address this challenge, this paper proposes \sys{}, a scheduling
system that associates with each function a \textit{\priceclass{}}, which is an
amount that it costs to run that function per unit time. Priority in the system
is directly paid for through \priceclass{}es, and all of the resource allocation
decisions in \sys{} are made on the basis of \priceclass{}.

\Sys{} has multiple goals it needs to achieve and challenges it needs to
address. One goal is that \sys{} needs to be able to support a multi tenant
environment. \Priceclass{}es achieve this: rather than dealing in a relative
ordering of developers' functions by latency sensitivity, which would be
difficult to compare across developers, the connection to money allows the
\class{} to have meaning on an absolute scale. It also incentivizes usage of the
lower \class{}es for functions that are less latency sensitive, since they can
be run cheaply.


Another important goal is that of placing functions quickly enough. For example,
a function that takes 20ms to run cannot spend 50ms in scheduler queues and
waiting for an execution environment before even starting to run. Knowing where
the free and idle resources are, or finding out quickly, is challenging in a
setting where both the number of new functions invocations and the amount of
resources are large.


Finally, a key challenge in designing \sys{} is that of managing memory. For
compute resources, cores can be timshared or processes preempted, but the buck
stops once a machine is out of memory. Current systems address this challenge by
requiring developers to express a maximal amount of memory they will use, and
charging based on that. However, memory usage is at best difficult to know in
advance and at worst has a large variance so is impossible to say in advance.
And more importantly, is not correlated with what developers actually care
about, which is function latency. Instead, \sys{} charges developers based on
the amount of memory actually used, and requires no bound to be set.~\Sys{} is
thus faced with the challenging proposition of blindly placing functions not
knowing how much memory they will use, but still needing memory utilization to
be high.
 

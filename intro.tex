%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

A world where all cloud compute is run in the format of a serverless job is
attractive to developers and providers: developers only pay for what they use
while having access to lots of resources; and cloud providers have flexibility
in scheduling and can achieve good utilization.

This paper focuses on building a usable and efficient priority mechanism for a
world of universal serverless, and assumes that cold start times are small
enough to be acceptable on the critical path --- recent state of the art systems
have been able to support cold start times in the single digit ms
range\cite{TODO}, and we expect this trend to continue. 


% However, for many applications it is rare to see them entirely hosted on
% serverless offerings today\cite{TODO}. 
A driving example for this work is that of a web server. Its characteristics of
unpredictable and bursty traffic make it well-suited for serverless, and the pay
as you go model is particularly attractive to website developers who don't want
to have to worry about provisioning.

Suppose a simple web server consists of two different page views, one for the
landing page and another for users' profile pages, and also has map reduce jobs
for data processing. 

If the web developer wanted to host the web server in an existing serverless
offering, a popular option is AWS lambda\cite{TODO}. In order to influence the
ways that different functions scale, AWS lets developers specify their desired
`reserved concurrency', ie the number of warm containers that are specifically
reserved for a given lambda\cite{TODO}. However, with that interface, the
developer loses the benefit of the flexibility that was the initial draw: they
are now paying for resources they are not using (because they have to pay to
keep all those containers warm), and they can't burst when they need to (because
once the number of invocations goes beyond the allocated warm containers it will
contend with all the other functions for warm instances, or have to wait for a
cold start).


% Spot instances are not a good fit: there are no guarantees about getting an
% instance, the instances take long to set up, and last for in the
% hours\cite{TODO}. A key characteristic of the serverless format is that there is
% a process per request, this is what ensures that the developer is only paying
% for what they need --- because they only get allocated resources when they need
% them, ie when a request comes in.


Designing a scheduling system that enforces priorities while maintaining a
purely on-demand usage structure faces some significant challenges.\hmng{this is
a bit of an awkward transition}


One of the challenges is that of multi-tenancy. Priorities alone are only
relative: the developer of our web server knows that they want landing page
views to run before profile page views to run before map reduce jobs; but have
no way of expressing that priority relative to other peoples jobs. And, in fact,
should not be trusted to: their own jobs will always seem more important than
others', so if given a scale their high priority work will always have the
highest priority available.

Another challenge is that of managing memory. Once a global ordering of all the
processes' priorities has been established, implementing (un)-fairness in
scheduling cpu time is somewhat straighforward. In a world with unlimited
memory, it would be simple to preempt profile view jobs with landing page view
jobs when they come in. However, in a memory-limited setting, it is not clear
how to know whether the profile view job might be need to be killed, and if so
whether it is worth it to do so, or better to just requeue the new job.

A third challenge is that of placing jobs. In a datacenter setting, where both
the number of new jobs coming in and the amount of resources are extremely
large, knowing where the free and idle resources are is a challenge central to
any distributed system. There is generally an understanding that the total load
does not fluctuate by any significant factor. This supported by the law of large
numbers (it is very unlikely that all a cloud providers customers will burst at
once), and can be seen in the ways AWS prices things: spot instances are sold at
a market rate determined by the amount of resources available in a given
zone\cite{TODO}, and the market rate is experientially very steady over
time\cite{TODO}; the prices for lambdas and ec2 instances also only changes very
slowly\cite{TODO}.

%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

A world where cloud compute is run in the format of serverless jobs is
attractive to developers and providers: developers pay only for what they use,
while having access to many resources when needed; and cloud providers have
control over scheduling and can use that to drive up utilization.



This characteristic of only paying for what you use is especially attractive to
developers of applications where the amount of resources that they need varies
significantly over time, or is generally small, so that buying their own
machines or renting a fixed amount (eg EC2) is untenable.

Some back of the envelope math shows that lambda functions can be cheaper: for a
low-traffic website, with approx 50K requests per day, a memory footprint of <
128 MB, and 200ms of execution, that adds up to \$1.58 per month. On the other
hand, the cheapest ec2 instance costs just over \$3 per month. Of course, as the
number of requests goes up, so does the price for lambda. There are pretty
extensive simulations that others have done that show the tradeoff points for
different types of workloads.
% https://www.bbva.com/en/innovation/economics-of-serverless/ - some nice graphs, pretty nuanced
% https://www.trek10.com/blog/lambda-cost - some good simple examples and a table
Lambdas can also burst faster: starting a new lambda execution environment is
much faster than starting a new container or ec2 instance, which can take
multiple minutes.
% https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-default-instance-warmup.html 



There are many reasons that developers choose not to use serverless, despite in
theory having workloads that are well-suited for the serverless environment.
% https://www.reddit.com/r/aws/comments/yxyyk3/without_saying_its_scalable_please_convince_me/
Some include cold start times, provider lock in, and lack of insight for
debugging and telemetry. The issue that this paper addresses is that of the
resource requirement interface, and how developers are/are not able to express
their needs for resources and latency.


The interface for expressing requirements is already one of the complications of
the above modeling: developers are required to choose for each function an
amount of memory they will need, which is then tied to a cpu power (an amount of
vCPUs). This means that the break-even point and general cost and performance
tradeoffs are heavily dependent on the details of the workload, including but
not limited to how much memory it will need, how long it will run, and whether
it is cpu or i/o bound. 

These things are often difficult to know in advance, and more importantly are
not correlated with what developers actually care about, which is latency. In
fact, measurements have found that in some ways the two are inversely
correlated, ie that lambdas that had more memory allocated took longer to start
up. 
% https://www.simplybusiness.co.uk/about-us/tech/2019/03/aws-lambda-cold-start/. 
This may or may not be outweighed by the speedup associated with more memory,
but knowing in advance how much your function will speed up given different
amounts of memory can be difficult.



This paper proposes a new interface, that centers priority rather than resource
requirements; and a scheduler \sys{} that enforces the priorities laid out. This
is getting at the heart of what serverless should be: a pay-per-use model, where
resource requirements don't need to be known ahead of time and are expected to
change. What is however known, and gets at the core of what developers care
about, is priority: some jobs are more important than others, and some clients
are willing to pay more money to run their jobs, and fast, than others. Priority
is explicitly tied to money in \sys{}: developers express priorities by choosing
price classes at which to run their functions.

Centering these classes benefits the developer as well as the provider. The
developer is not required to make estimates that may or may not end up being
correct, and can just pay for the resources they use. They have access to
cheaper options if they are willing to wait, and can skip the line and get high
priority access if they pay for it.

Providers, on the other hand, have a concrete metric that can allow them to
drive up utilization and profit: they can run the most expensive jobs first, and
fill utilization gaps with cheaper jobs. 

This also means that there are no clear guarantees when a developer puts a price
on a function they want to run. In order to mitigate that somewhat and not go
into bidding wars, we propose exposing a fixed set of price classes. This is
similar to how AWS has different EC2 instance types, that are directly mapped to
prices. 

Rather than being a guarantee, the price class is instead a metric to express
priority to \sys{}, which it can then use to enforce a favoring of high priority
jobs. It is a small change in the API, that still gives the scheduler the
information it needs to decide what to schedule when, and aligns the interests
of the developer with those of the provider more directly, by communicating on
the level of what the provider and developer actually care about: money and
latency, as achieved by priority in the system.


\hmng{nowhere in this new intro do I mention memory or challenges; guess its more 
of a motivation? should that come next?}


Designing a scheduler that enforces priorities faces multiple challenges. 

One of the challenges is that of placing jobs quickly enough. For example, a job
that takes 20ms to run cannot spend 100ms in scheduler queues and waiting for an
execution environment before even starting to run. We focus on the aspect of
scheduling latency: in a datacenter, where both the number of new jobs coming in
and the amount of resources are extremely large, the challenge is knowing where
the free and idle resources are, or finding out quickly. For this paper, we
assume that that cold start times are small enough that cold starts on the
critical path are acceptable.\footnote{Cold start latencies have been reduced
significantly --- recent state of the art systems have been able to support
latencies in the single digit ms range\cite{TODO}.}

Another main challenge is that of managing memory. Placing a new high priority
job on a machine already running many jobs may lead to enough memory pressure to
require killing an already placed job (rather than the new job just preempting
existing jobs). When placing new job it is difficult to know whether the machine
would have to kill and requeue something else, and if so whether it is worth it, or better
to just requeue the new job.
 

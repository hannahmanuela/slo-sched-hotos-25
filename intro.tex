%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

A world where all cloud compute is run in the format of a serverless job is
attractive to developers and providers: developers only pay for what they use
while having access to lots of resources; and cloud providers have flexibility
in scheduling and can achieve good utilization.

This paper focuses on building a usable and efficient priority mechanism for a
world of universal serverless, and assumes that cold start times are small
enough to be acceptable on the critical path --- recent state of the art systems
have been able to support cold start times in the single digit ms
range\cite{TODO}, and we expect this trend to continue. 


% However, for many applications it is rare to see them entirely hosted on
% serverless offerings today\cite{TODO}. 
A driving example for this work is that of a web server. Its characteristics of
unpredictable and bursty traffic make it well-suited for serverless, and the pay
as you go model is particularly attractive to website developers who don't want
to have to worry about provisioning.

Suppose a simple web server consists of two different page views, one for the
landing page and another for users' profile pages, and also has map reduce jobs
for data processing. 

If the web developer wanted to host the web server in an existing serverless
offering, a popular option is AWS lambda\cite{TODO}. In order to influence the
ways that different functions scale, AWS lets developers specify their desired
`reserved concurrency', ie the number of warm containers that are specifically
reserved for a given lambda\cite{TODO}. However, with that interface, the
developer loses the benefit of the flexibility that was the initial draw: they
are now paying for resources they are not using (because they have to pay to
keep all those containers warm), and they can't burst when they need to (because
once the number of invocations goes beyond the allocated warm containers it will
contend with all the other functions for warm instances, or have to wait for a
cold start).


% Spot instances are not a good fit: there are no guarantees about getting an
% instance, the instances take long to set up, and last for in the
% hours\cite{TODO}. A key characteristic of the serverless format is that there is
% a process per request, this is what ensures that the developer is only paying
% for what they need --- because they only get allocated resources when they need
% them, ie when a request comes in.


Designing a scheduling system that enforces priorities while maintaining a
purely on-demand usage structure faces some significant challenges.\hmng{this is
a bit of an awkward transition}


One of the challenges is that of multi-tenancy. Priorities alone are only
relative: the developer of our web server knows that they want landing page
views to run before profile page views to run before map reduce jobs; but have
no way of expressing that priority relative to other peoples jobs. And, in fact,
should not be trusted to: their own jobs will always seem more important than
others', so if given a scale their high priority work will always have the
highest priority available.

Another challenge is that of managing memory. Supposing no multi-tenancy and
unlimited memory, it would be simple to preempt profile view jobs with landing
page view jobs when they come in. However, in a memory-limited setting, it is
not clear how to know whether running the landing page view would mean the
profile view job would be need to be killed, and if so whether it is worth it to
do so, or better to just requeue the new job.

A third challenge is that of placing jobs. In a datacenter setting, where both
the number of new jobs coming in and the amount of resources are extremely
large, knowing where the free and idle resources are is a challenge central to
any distributed system. Because the scheduler has access to priority information
about processes, this can give the scheduler a notion of how quickly things will
need to be placed, but not how many resources they will use. It also interacts
with the memory management challenge: placing a job requires balancing the
tradeoffs between potentially needing to kill already running jobs, or putting
the new job on a machine where it may have to wait for other high priority jobs
to finish. Additionally, if a machine will preempt low priority processes with
high priority ones, then it may even be desirable to place a high priority job
on a machine with a longer queue (as long as they are mostly low priority jobs),
rather than on a machine with few high priority processes.\hmng{this challenge
definitely feels the least worked out}


% There is generally an understanding that the total load does not fluctuate by
% a significant factor. This supported by the strong law of large numbers (it is
% very unlikely that all the jobs' random  bursts align), and can be seen in the
% ways AWS prices things: spot instances are sold at a market rate determined by
% the amount of resources available in a given zone\cite{TODO}, and the market
% rate is experientially very steady over time\cite{TODO}. The prices for
% lambdas and ec2 instances also only changes very slowly\cite{TODO}.

\hmng{we also never get back to these challenges yet. Maybe have discussion
section centered around them? The existing discussion `subsections'
could probably be made to fit into the challenges}

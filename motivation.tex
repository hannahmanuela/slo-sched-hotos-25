%-------------------------------------------------------------------------------
\section{Motivation}\label{motivation}
%-------------------------------------------------------------------------------

This paper is motivated by the benefits of serverless as an approach to utility
computing, and finds latency variability to be a key challege in making true
serverless a reality.

\subsection{Benefits of serverless}

The main attraction of serverless for developers is, in an idealized world, the
characteristic of paying only for what they use, while having a whole datacenter
available to them. This proposition is especially attractive to developers of
applications where the amount of resources that they need varies significantly
over time, or is generally small and very spread out. With such workloads,
buying their own machines or renting a fixed amount of server space is bad for
the developer because it is expensive if provisioned for peak usage and has poor
performance if not, and bad for providers because it leads to low utilization.

A central example to this paper is that of a web application. Its traffic
patterns make it a great candidate for running as serverless functions: it is
event-based, its load is bursty and unpredictable, and a request's resource
requirements can vary greatly depending on which user invoked it.


A back of the envelope calculation shows that for web applications with small
load, lambda functions as they stand today are cheaper: for a low-traffic
website, with approx 50K requests per day, a memory footprint of < 128 MB, and
200ms of execution, running that on AWS lambda adds up to \$1.58 per month. On
the other hand, the cheapest EC2 instance costs just over \$3 per month. Of
course, as the number of requests goes up, the price for lambdas scales
linearly, whereas running an EC2 instance on full load becomes comparably cheap.
Extensive simulations show a more nuanced picture of the tradeoff points for
different workloads~\cite{econ-of-serverless,trek10-blog}.

Serverless also may outperform reservation systems for workloads that are very
bursty: starting a new lambda execution environment is much faster than starting
a new container or EC2 instance, which can take multiple
minutes~\cite{ec2-autoscaling}.


\subsection{Challenge of latency variability}


\begin{figure}[t!]
    \centering
      \includegraphics[width=8.5cm]{img/lambda_total_durations.png}
      \caption{ distribution of end to end duration times. The y axis is log scale }
    \label{fig:lambda-total-durations}
\end{figure}

However, only few web applications run entirely on serverless offerings today.
There are many reasons that developers choose not to use serverless, despite
their workloads being well-suited for the serverless environment. Popular
complaints include provider lock in, lack of insight for debugging and
telemetry, and variable runtimes~\cite{not-lambda-blog,reddit-serverless2}.


\Sys{} focuses on the challenge of variable runtimes. It must be the case that
any system that runs with high average utilization experiences moments where
there is more load than the resources can handle. As we know from recent
traces~\cite{prequal}, although load may look stable at a time increment of
hours or even minutes, going down to the second level shows the load to have
high variance. If providers want to have good average utilization, then in the
moments of load spike the load will be more than the resources can handle.

In a multi tenant setting, what this realistically means is that one
developers exprienced latencies may well depend on other peolpe's load. This is
not ideal, expeicially if one developer's user facing function is queued while
another developers background task is running.

We can see variable invocation times happening in lambda invocations today, 
within the cold start invocations. We run an experiment with a simple lambda
function that sleeps for 20ms and then returns. We use AWS Xray~\cite{aws-xray}
to measure its latency, with incovations spaced randomly between 0 and 10
minutes. The results are in Figure~\ref{fig:lambda-total-durations}. The spike
on the left side of the graph is the execution times from invocations that used
warm start. The durations remain stable, because AWS is able to simply route the
new request to the machine with the existing container on it. We verify this
what is happening by changing the function to include reading and then writing
to an environment variable, and find that for invocations with warm start when
we read the variable it was already set by a previous invocation.\hmng{We don't
know what happens if the machine with the warm container on it is currently
already fully loaded though}

The right grouping in the graph is those invocations that hit cold starts, whose
overall latencies vary between $\sim$200 and $\sim$400ms. This is the variance
we care about, because this is the path we expect most invocations to take as
cold starts get cheaper? Because not everything can be warm start so this needs
to be acceptable to page views as well? The container we used was a generic AWS
python runtime, which excludes latency factors like downloading the runtime.

Although AWS' scheduling mechanism is proprietary, we can look to open source
alternatives to see what common best practices exist today. Schedulers have two
different options when load exceeds compute capacity: queue the excess load, or
place it on machines and let them deal with being temporarily overloaded.
Different schedulers have different approaches. In OpenWhisk~\cite{openwhisk},
the load balancer will choose which machine to run the function on, and then
place the invocation, adressed to that machine, into a Kafka queue that the
machine subscribes to and can pull the invocation from when it is
ready~\cite{openwhisk-sched}. Knative~\cite{knative} similarly queues the excess
invocations, although it does so via the load balancer, which is also in charge
of autoscaling~\cite{knative-sched}: if the existing pods are fully loaded (with
a small, bounded-size queue in front of them), requests are queued separately
while the autoscaler starts up more invocations.

Hermod~\cite{hermod} is a recent research serverless scheduler, and shows in a
simulation that late binding (as openwhisk and knative do) performs worse than
early binding. Under high load, Hermod places the excess functions on machines
anyway (used a least loaded policy) and does Processor Sharing scheduling among
them. This ensures that no one function has a high delay, but rather that all
are equally slowed down. What the paper does not address, however, is what
happens when the machines are out of memory. 

Because none of these schedulers have information about the functions they are
queueing, it is impossible for them to know which to prioritize. Without further
tooling, in a moment of high load that requires queueing/delaying, latency
sensitive jobs might end up in the queue behind background functions. The way
all of the above schedulers avoid this problem is by doing some sort of
accounting of concurrency: concurrency can be reserved or provisioned for
specific functions, and limited for others. This is necessary to ensure that a
burst in background tasks doesn't starve the latency sensitive functions.
However, what happens when the datacenter is out of resources and so the
concurrency limit has not yet been reached but the resources are unavailable is
not clear. Reserving and provisioning and limiting are also conceptually in
tension with the goal of serverless, which is to be on-demand and flexible.



% It is impossible to know what proprietary systems like AWS do, but since AWS
% guarantees an amount of memory as well as a fraction of vCPUs to each function,
% it is likely AWS also queues the invocations to ensure they aren't put in a
% position of having to break those guarantees.\hmng{that might be a test we could
% run: do a tight while loop and look at cpu time and wall clock time to see how
% we are being scheduled} 



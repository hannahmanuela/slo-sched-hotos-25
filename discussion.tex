%-------------------------------------------------------------------------------
\section{Discussion}
%-------------------------------------------------------------------------------


There remain a number of interesting open questions.

\textbf{How is memory usage billed?}
% 
Do developers only pay for the memory they use, or do they pay based on the max
amount of the memory they stupilate? The former would result in gross
overestimations because there is no downside to being on the cautious side,
while the latter breaks the central maxim that developers only pay for what they
use.

In the world where developers only pay for what they use, asking for a limit
with no repurcussions if the estimate is off would be useless; everyone would
just put the largest amount allowed. Adding penalties for being off seems
unreasonable: some jobs simply have a high variance in their memory usage and
penalizing developers that run such jobs is undesirable. 

Ideally, memory would be a pay-per-use model and no further information would be
requied; however that would require being able to reactively deal with memory
pressure extremely quickly and well.

\textbf{How should \sys{} deal with memory pressure?} 
% 
Ideally, \sys{} would avoid the situation altogether, or if it occured would be
able to solve it without killing, ie wasting resources. 

Avoiding memory pressure situations would require knowing with high accuracy in
advance how much memory a job will use. Profiling is improving as ML models
improve, and might be a good use case (especially for jobs with a high variance
of memory usage, using a model that is given the inputs to invocations could
work well, since the inputs are likely to be the determining factor for memory
usage). 

On the other hand, profiling is still just an estimate that could always
be wrong, and the system needs to be able to handle that. If we are able to come
up with a mechanism that is reactive and at the right time scale, ie acts
quickly enough that there is no buildup and the problem goes away, that would be
ideal.

One option for this might be snapshotting: lower priority functions that we are now
reactively killing might be allowed to snapshot themselves and then be placed
somewhere else and re-started. In this case, the timescale would have to be such
that the snapshotting does not (in its use of memory or compute) prohibively
block the other jobs on the machine from running. Another option could be
paging: the lower priority processes' memory are paged out; later when there is
lower load and they start running again their latency will be affected but they
are lower priority so we don't care as much (since developers pay per usage for
compute one could imagine some sort of recompense for the runtime that job pays
for having been paged out).


\textbf{Does priority increase with waiting?}
% 
Preemptive priority scheduling means that at any moment in time the process
running is the one with the highest priority on the machine. This ensures that
the web server's page views run with high performance, and the map reduce jobs
do not interrupt but rather wait for lulls in load to run. The flipside of this
is that if there are no such lulls, ie if there are so many high priority jobs
that they take up all the resources all the time, it is possible that nothing of
any lower priority would ever run at all. 

One solution to this problem is ensure that it can never be the case the there
is so much load on high priority jobs that the data center will be full with
them to such a degree that high-ish priority jobs can't run. There is evidence
for and we expect that load will mostly be stable: this supported by the strong
law of large numbers (it is very unlikely that all the jobs' random  bursts
align), and can be seen in the ways AWS prices things: spot instances are sold
at a market rate determined by the amount of resources available in a given
zone\cite{TODO}, and the market rate is experientially very steady over
time\cite{TODO}. The prices for lambdas and ec2 instances also only changes very
slowly\cite{TODO}. This means that providers can mostly choose the rough
breakdown of the load they will have at any given time (ie they can choose a
percentage of how many jobs they want in each priority, and change it by
adjusting prices or not sllowing users to select that level anymore). 

\hmng{We discuss what a good breakdown would look like in the next section? Put eval
next? Or just don't discuss and leave it at that, although that seems a little
vague.}



%-------------------------------------------------------------------------------
\section{Discussion}
%-------------------------------------------------------------------------------



There are still a number of open questions that the design only partially addresses, or
is not completely satisfactory about.

\textbf{Economics of memory.}
One of the key pieces of the design is the notion that jobs have an associated
amount of memory, and when the job is placed that estimate is used in order to
understand the consequences of placing a job (whether this will result in a kill
of not). However, we did not discuss the payment model: do developers only pay
for the memory they use, or do they pay based on the max amount of the memory
they give? The former would result in gross overestimations because there is no
downside to being on the cautious side, while the latter breaks the central
maxim that developers only pay for what they use.

In the world where developers only pay for what they use, asking for a limit
with no repurcussions if the estimate is off would be useless; everyone would
just put the largest amount allowed. Adding penalties for being off seems
unreasonable: some jobs simply have a high variance in their memory usage and
penalizing developers that run such jobs is undesirable.

\textbf{Dealing with memory pressure.}
Whether the meory usage per job is capped or not, because usage varies having
high memory utilization will requite overprovisioning machines on memory. The
current design deals with high memory pressure by simply killing the process
that is the most convenient: lowest priority and using enough memory to make a
difference.

This is not really a satisfying solutiong. Ideally, the system would avoid the
situation altogether, or if it occured would be able to solve it without
killing, ie wasting resources. 

Well-know solutions for this are profiling or reactively snapshotting/paging.
Profiling is improving as ML models improve, and might be a good use case
(especially for jobs with a high variance of memory usage, using a model that is
given the inputs to invocations could work well, since the inputs are likely to
be the determining factor for memory usage). On the other hand, profiling is
still just an estimate that could always be wrong, and the system needs to be
able to handle that. If we are able to come up with a mechanism that is reative
and at the right time scale, ie acts quickly enough that there is no buildup and
the problem goes away, would be ideal. \\
One option for this might be snapshotting: lower priority functions that we are now
reactively killing might be allowed to snapshot themselves and then be placed
somewhere else and re-started. In this case, the timescale would have to be such
that the snapshotting does not (in its use of memory or compute) prohibively
block the other jobs on the machine from running. Another option could be
paging: the lower priority processes' memory are paged out; later when there is
lower load and they start running again their latency will be affected but they
are lower priority so we don't care as much (since developers pay per usage for
compute one could imagine some sort of recompense for the runtime that job pays
for having been paged out).


\textbf{Does priority increase with waiting.}
One of the observations the system design is built on is that Processor Sharing
(PS) is not the right approach in a serverless setting, where jobs run once and
then are done (as opposed to long running processes that need consistent access
to resources). Instead, \sys{} uses preemptive priority scheduling, where
at any moment in time the process running is the one with the highest priority
on the machine. This means that if it is possible for load in the highest
priority class to take up all the resources, nothing of any lower resource would
ever run at all. 

It is not necessarily clear that this is desirable, for instance in the web
server example if a user profile view has waited for long enough it seems fine
for a landing page view to be blocked for a second while the profile view job
runs. This would point to it being desirable that processes gain priority as
they wait. On the other hand, this would not be true of a map reduce job: page
views should never be interrupted because a map review job had been waiting for
a long time.

One solution to this problem is ensure that it can never be the case the there
is so much load on high priority jobs that the data center will be full with
them to such a degree that high-ish priority jobs can't run. As we said earlier,
there is evidence for and we expect that load will mostly be stable. Random
short bursts occur for different clients but generally not related (or when they
are there is a cause and cloud providers will know the cause and be able to
expect it, for example live stream web servers having higher load during the
superbowl). This means that providers can mostly choose the rough breakdown of
the load they will have at any given time (ie they can choose a percentage of
how many jobs they want in each priority, and change it by adjusting prices or
not sllowing users to select that level anymore). 

We discuss what a good breakdown would look like in the next section? Put eval
next? Or just don't discuss and leave it at that, although that seems a little
vague.



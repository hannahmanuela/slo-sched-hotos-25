%-------------------------------------------------------------------------------
\section{Discussion}
%-------------------------------------------------------------------------------

In this section we discuss the the way that having priorities would impact how
we can approach some of the open questions that remain about how a scheduler
should best manage resources.


\subsection{Managing memory}

In section~\ref{design}, \sys{} makes decisions based on maximum memory usage
estimates for each job, provided by the developer. However, in reality, making
scheduling decisions based off of this sort of estimate of a maximum can lead to
underutilization. Jobs often have highly variable memory usage; for instance the
distribution of content among users of social media platforms is notoriously
skewed\cite{TODO}, so jobs that fetch information for a user and process it will
be run over very different amounts of data depending on who the user is. So
making scheduling decisions based off of an estimate of a maximum will lead to
memory underutilization, and instead we are forced to play a game of
overprovisioning.

Having priorities can help. We overprovision in order to achieve high
utilization, so need a way of dealing with a situation where there are not
enough resources for all the jobs on a machine. Under that situation of high
memory pressure, we suggest a priority-based paging approach, where the lower
priority jobs' memory are paged out. Because machines run highest priority first
scheduling, we know that the paged job will not be run until the higher priority
jobs have finished (and thus freed their memory).\hmng{there's a caveat here
where they could block on i/o, but the likelihood that all the jobs except the
last one block on i/o is low} Later, when there is lower load, and the
previously paged job is ready to run again, the machine can page back in all of
its memory. Thus the paged job pays no latency overhead for having been paged,
and the amount of time spent paging memory is low: once to move all the memory
out, and once to move it back in. This is the minimal amount of data movement
necessary to free up the required memory without deleting/killing.


% One option is profiling: as ML models improve, so our ability to predict things
% improves, including predicting memory usage. Especially for jobs with a high
% variance of memory usage, using a model that is given the inputs to invocations
% could work well, since the inputs are likely to be the determining factor for
% memory usage (for instance, the model could learn which users are the ones that
% have huge amounts of data). However, predictions are still just estimates that
% could always be wrong, and the system needs to be able to handle that. They also
% need to be based on profiling information, which will be slow to adapt to
% changes in code and in the underlying data.

% Another option is snapshotting: lower priority functions are allowed to snapshot
% themselves and then are placed somewhere else and re-started. This way none of
% their already done computation is wasted, and the memory is freed. However, the
% timescale would have to be such that the snapshotting does not (in its use of
% memory or compute) prohibitively block the other jobs on the machine from
% running. This works against the incentives of the situation: ideally we want to
% free a large amount of memory, but the time it takes to snapshot a job scales
% with the amount of memory it is using.


\subsection{Managing compute}

We have already seen that having priorities helps \sys{} deal with
overprovisioning on the compute side. Traditionally, managing the response time
for important jobs requires the overall load on the system to be low, because
slowdown is averaged across all the jobs that can share resources. However,
having priorities allows \sys{} to be targeted about how compute resources are
shared: rather than averaging out the slowdown caused by overprovisioning across
all the jobs by having them time share the cores, \sys{} can use priorities to
decide which job has to wait. This means that in priority scheduling the amount
of time a job spends waiting for resources is only defined by the load of jobs
with equal or higher priority.

The flipside of this is that it is possible that the entire load of the
datacenter will be such that low priority jobs are starved. This is acceptable
and in fact desirable for small amounts of time, but keeping this effect in
check requires managing the overall load. 

We propose to solve this problem by ensuring that it can never be the case the
there is so much load on high priority jobs that the data center will be full
with them to such a degree that other jobs can't run. There is evidence for and
we expect that load on a high level will mostly be stable, with diurnal and
annual patterns.\cite{TODO}

This means that providers can mostly choose the rough breakdown of the load they
will have at any given time (ie they can choose a percentage of how many jobs
they want in each priority, and change it by adjusting prices or not sllowing
users to select that level anymore). 

\hmng{We discuss what a good breakdown would look like in the next section? Put eval
next? Or just don't discuss and leave it at that, although that seems a little
vague.}



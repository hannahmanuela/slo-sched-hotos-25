%-------------------------------------------------------------------------------
\section{Discussion}
%-------------------------------------------------------------------------------

\subsection{Memory}

In section\ \ref{design}, \sys{} makes decisions based on maximum
memory usage estimates for each job, provided by the developer. However, in
reality, making scheduling decisions based off of this sort of estimate of a
maximum can lead to underutilization. Jobs often have highly variable memory
usage; for instance the distribution of content among users of social media
platforms is notoriously skewed\cite{TODO}, so jobs that fetch information for a
user and process it will be run over very different amounts of data depending on
who the user is.

So we are forced to either get better estimates, or play a game of
overprovisioning, because reserving the maximum amount of memory would lead to,
in most cases, low memory utilization.\hmng{who is `we' now? it's not \sys{},
but it's also not some other concrete system, it's kinda the hypothetical one
we're discussing}

One option is profiling: as ML models improve, so our ability to predict things
improves, including predicting memory usage. Especially for jobs with a high
variance of memory usage, using a model that is given the inputs to invocations
could work well, since the inputs are likely to be the determining factor for
memory usage (for instance, the model could learn which users are the ones that
have huge amounts of data). However, predictions are still just estimates that
could always be wrong, and the system needs to be able to handle that. They also
need to be based on profiling information, which will be slow to adapt to
changes in code and in the underlying data.

Another option is snapshotting: lower priority functions are allowed to snapshot
themselves and then are placed somewhere else and re-started. This way none of
their already done computation is wasted, and the memory is freed. However, the
timescale would have to be such that the snapshotting does not (in its use of
memory or compute) prohibitively block the other jobs on the machine from
running. This works against the incentives of the situation: ideally we want to
free a large amount of memory, but the time it takes to snapshot a job scales
with the amount of memory it is using.
 
A stronger option is priority-based paging: the lower priority processes' memory
are paged out. Because machines run highest priority first scheduling, we know
that the paged job will not be run until the higher priority processes have
finished (and thus freed their memory).\hmng{there's a caveat here where they
could block on i/o, but the likelihood that all the processes except the last
one block on i/o is low} Later, when there is lower load, and the previously
paged job is ready to run again, the machine can page back in all of its memory.
Thus the paged job pays no latency overhead for having been paged, and the
amount of time spent paging memory is low: once to move all the memory out, and
once to move it back in. This is the minimal amount of data movement necessary
to free up the required memory without deleting/killing.


\subsection{Compute}

\sys{} uses preemptive priority scheduling on the machine scheduler level. This
means that at any moment in time, the process currently running is the one with
the highest priority on the machine. This ensures that the web server's page
views run with high performance, and the map reduce jobs do not interrupt but
rather wait for lulls in load to run. The flipside of this is that if there are
no such lulls, ie if there are so many high priority jobs that they take up all
the resources all the time, it is possible that nothing of any lower priority
would ever run at all.

We propose to solve this problem by ensuring that it can never be the case the
there is so much load on high priority jobs that the data center will be full
with them to such a degree that other jobs can't run. There is evidence for and
we expect that load on a high level will mostly be stable, with diurnal and
annual patterns.\cite{TODO}

This means that providers can mostly choose the rough breakdown of the load they
will have at any given time (ie they can choose a percentage of how many jobs
they want in each priority, and change it by adjusting prices or not sllowing
users to select that level anymore). 

\hmng{We discuss what a good breakdown would look like in the next section? Put eval
next? Or just don't discuss and leave it at that, although that seems a little
vague.}



%-------------------------------------------------------------------------------
\section{Preliminary Results}
%-------------------------------------------------------------------------------



In order to understand the case for \sys{}, we ask the following questions: 
\begin{enumerate}
    \item How does job latency in \sys{} compare to schedulers without
    priorities on one hand, and theoretically optimal schedulers with perfect
    information on the other?
    \item Does \sys{}'s plan for managing memory work?
\end{enumerate}


To explore these questions, we built a simulator in go\cite{TODO}, which
simulates different scheduling approaches.

\begin{figure*}[t!]
    \centering
      \includegraphics[width=16cm]{img/hermod_xx_edf_latencies.png}
      \caption{ add caption }
    \label{fig:hermod-xx-edf}
\end{figure*}


\subsection{Experimental Setup}

In each version of the simulator, jobs arrive in an open loop at a constant
rate. The simulator attaches three main characteristics to each job it
generates: runtime, \priceclass{}, and memory usage.\ \textit{Job runtime} is
chosen by sampling from randomly generated long tailed (in this case pareto)
distribution: the relative length of the tail ($\alpha$ value) remains constant,
and the minimum value ($x_m$) is chosen from a normal distribution. This
reflects the fact that different functions have different expected runtimes
(chosen from a normal distribution), and that actual job runtimes follow long
tailed distributions (so each pareto distribution that we sample represents the
expected runtime distribution of a given function).\ \textit{Job \class{}} is
chosen randomly, but weighted: the simulator uses a vaguely bimodal weighting
across priorties. The simulator has n different \priceclass{} values, each
assigned to a fictitious price. Because functions are randomly assigned a
\class{}, runtime and \class{} are not correlated.\ \textit{Job memory usage} is
chosen randomly between 100MB and 10GB. Over their lifetime, jobs increase their
memory usage from an initial amount (always 100MB) to their total usage.
% from a bimodel normal distribution, with peaks at 1GB and at 8GB. 

When comparing two different simulated schedulers, they each are given an
identical workload and then each simulate running that workload.

The simulator makes some simplifying assumptions:
\begin{enumerate}
    \item functions are compute bound, and do not block for i/o
    \item communication latencies are not simulated
    \item the amount of time it takes to swap memory is not simulated
\end{enumerate}

We simulate running 100 machines, with 8 cores and 32GB RAM each, with 4
scheduler shards and a k-choices value of 3 when applicable.

\subsection{How do job latencies compare?}

In the end developers care about job latency, so it is important to understand
how well \priceclass{}es do at reflecting and enforcing SLAs. On one hand, is
relevant to understand if we need \class{}es at all: is there a scheduler that
can, without having any access to information about which jobs are important,
still ensure that jobs perform well? On the other hand, it is helpful to compare
\sys{} to an ideal scheduler, in order to contextualize \sys{}'s performance.

To explore one side of this question, we look at how an existing state of the
art research scheduler that does not take any form of priority into account
performs. We simulate Hermod\cite{TODO}, a state-of-the-art serverless scheduler
built specifically for serverless. Hermod's design is the result of a
from-first-principles analysis of different scheduling paradigms. In accordance
with the paper's findings, we simulate least-loaded load balancing over machines
found using power-of-k-choices, combined with early binding and Processor
Sharing machine-level scheduling. Hermod does not use priorities in its design,
and as such the simulator ignores jobs' \class{} when simulating Hermod's
design.


On the other side, we want to simulate an ideal scheduler. Ideal here is with
respect to meeting jobs' SLAs, which requires defining the desired SLA. In order
to do this, we assign each job invocation a deadline, and allow the deadline to
be a function of the job's true runtime. We define the deadline as a function of
the runtime as well as the \priceclass{}, as follows: deadline = runtime *
maxPrice/price (as if each process were weighted by its price). This ensures
that highest \class{} jobs' deadlines are simply their runtimes, and deadlines
get more and more slack with lower \class{}es. We then simulate an Earliest
Deadline First (EDF) scheduler over these deadlines, which is queuing
theoretically proven to be optimal in exactly the way we wanted: if it is
possible to create a schedule where all jobs meet their deadline, EDF will find
it\cite{TODO}.
% https://en.wikipedia.org/wiki/Earliest_deadline_first_scheduling

We compare the latencies observed in both of these settings with those that
running \sys{} produces. Because Hermod does not talk about dealing with memory
pressure, and to avoid an unfair comaprison with \sys{}'s swapping, we set the
memory to be absurdly high for all three settings in this experiment. We also
turn off the use of the idle list in \sys{}, so as to be en par with Hermod in
placing load, and revert solely to k-choices.

A strong result for \sys{} would show that its performance is between the two,
and closer to the EDF side. Especially as load and utillization get high, we
expect that the differences betweeen the three approaches will become evident.
Figure~\ref{fig:hermod-xx-edf} shows the results. TODO write something once we
like the graph


\subsection{Does \sys{} plan for memory management work?}

\begin{figure}[t!]
    \centering
      \includegraphics[width=8cm]{img/memory_graphs.png}
      \caption{ add caption }
    \label{fig:memory-graphs}
\end{figure}

To answer this question, we look at how \sys{} distributes load, and whether the
amount that \sys{} needs to swap memory is realistic. We now run \sys{} in a
setting of limited memory, and track the memory utilization of different
machines, as well as how much and what they need to swap. A good result would
show a tigh spread of memory utilization, that machines only start swapping once
memory utilization is high, and that the amount of swapping being done is also
equally spread across machines. Figure~\ref{fig:memory-graphs} shows the
results.



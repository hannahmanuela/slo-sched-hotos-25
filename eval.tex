%-------------------------------------------------------------------------------
\section{Preliminary Results}
%-------------------------------------------------------------------------------



In order to understand the case for \sys{}, we ask the following questions: 
\begin{enumerate}
    \item How does job latency in \sys{} compare to schedulers without
    priorities on one hand, and theoreticlaly optimal schedulers with perfect
    information on the other?
    \item Does \sys{} plan for managing memory work?
\end{enumerate}


To explore these questions, we built a simulator in go\cite{TODO}, which
simulates different scheduling approaches.


\subsection{Experimental Setup}

In each version of the simulator, jobs arrive in an open loop at a constant
rate. The simulator attaches three main characteristics to each job it
generates: runtime, priority, and memory usage.\ \textit{Job runtime} is chosen
by sampling from randomly generated long tailed (in this case pareto)
distribution: the relative length of the tail ($\alpha$ value) remains constant,
and the minimum value ($x_m$) is chosen from a normal distribution. This
reflects the fact that different functions have different expected runtimes
(chosen from a normal distribution), and that actual job runtimes follow long
tailed distributions (so each pareto distribution that we sample represents the
expected runtime distribution of a given function).\ \textit{Job priority} is
chosen randomly, but weighted: the simulator uses a vaguely bimodal weighting
across priorties. The simulator has n different priority values, each assigned
to a fictitious price. Because functions are randomly assigned a priority,
runtime and priority are not correlated.\ \textit{Job memory usage} is chosen
randomly between 100MB and 10GB.
% from a bimodel normal distribution, with peaks at 1GB and at 8GB. Over their
% lifetime, jobs increase their memory usage from an initial amount (always 100MB)
% to their total usage.

When comparing two different simulated schedulers, they each are given an
identical workload and then each simulate running that workload.

The simulator makes some simplifying assumptions:
\begin{enumerate}
    \item functions are compute bound, and do not block for i/o
    \item communication latencies are not simulated
    \item amount of time it takes to page memory is not simulated
\end{enumerate}

We simulate running 100 machines, with 8 cores and 32GB RAM each, with 4
scheduler shards and a k-choices value of 3 when applicable.

\subsection{How do job latencies compare?}

\begin{figure}[t!]
    \centering
      \includegraphics[width=8cm]{img/hermod_xx_edf_latency.png}
      \caption{ add caption }
    \label{fig:hermod-xx-edf}
\end{figure}

In the end developers care about job latency, so it is important to understand
how well priorities do at reflecting and enforcing SLAs. On one hand, is
relevant to understand if we need priorities at all: is there a scheduler that
can, without having any access to information about which jobs are important,
still ensure that jobs perform well? On the other hand, it is helpful to compare
\sys{} to an ideal scheduler, in order to contextualize \sys{}'s performance.

To explore one side of this question, we look at how an existing state of the
art research scheduler that does not take any form of priority into account
performs. We simulate Hermod\cite{TODO}, a state-of-the-art serverless scheduler
built specifically for serverless. Hermod's design is the result of a
from-first-principles analysis of different scheduling paradigms. In accordance
with the paper's findings, we simulate least-loaded load balancing over machines
found using power-of-k-choices, combined with early binding and Processor
Sharing machine-level scheduling. Hermod does not use priorities in its design,
and as such the simulator ignores jobs' priority when simulating Hermod's
design.


On the other side, we want to simulate an ideal scheduler. Ideal here is with
respect to meeting jobs' SLAs, which requires defining the desired SLA. In order
to do this, we assign each job invocation a deadline, and allow the deadline to
be a function of the job's true runtime. We define the deadline as a function of
the runtime as well as the priority, as follows: deadline = runtime *
maxPrice/price (as if each process were weighted by its price). This ensures
that highest priority jobs' deadlines are simply their runtimes, and deadlines
get more and more slack with lower priorities. We then simulate an Earliest
Deadline First (EDF) scheduler over these deadlines, which is queuing
theoretically proven to be optimal in exactly the way we wanted: if it is
possible to create a schedule where all jobs meet their deadline, EDF will find
it\cite{TODO}.
% https://en.wikipedia.org/wiki/Earliest_deadline_first_scheduling

We compare the latencies observed in both of these settings with those that
running \sys{} produces. Because Hermod does not talk about dealing with memory
pressure, and to avoid an unfair comaprison with \sys{}'s paging, we set the
memory to be absurdly high for all three settings in this experiment. We also
turn off the use of the idle list in \sys{}, so as to be en par with Hermod in
placing load, and revert solely to k-choices.

A strong result for \sys{} would show that its performance is between the two,
and closer to the EDF side. Especially as load and utillization get high, we
expect that the differences betweeen the three approaches will become evident.
Figure~\ref{hermod-xx-edf} shows the results. As expected, \sys{} outperforms
Hermod once the utilization is above YYY, because Hermod spreads the slowdown
across all the jobs, whereas \sys{} can be more targeted.~\sys{} performs
comparably to EDF, until very high load. This is because EDF, because it has
knowledge of jobs' approximate runtime, stops running the longer low priority
jobs, and thus can finish the low priority jobs it does start more quickly.


\subsection{Does \sys{} plan for memory management work?}

\begin{figure}[t!]
    \centering
      \includegraphics[width=8cm]{img/memory_graphs.png}
      \caption{ add caption }
    \label{fig:memory-graphs}
\end{figure}

To answer this question, we look at how \sys{} distributes load, and whether the
amount that \sys{} needs to page memory is realistic. We now run \sys{} in a
setting of limited memory, and track the memory utilization of different
machines, as well as how much and what they need to page. A good result would
show a tigh spread of memory utilization, that machines only start paging once
memory utilization is high, and that the amount of paging being done is also
equally spread across machines. Figure~\ref{fig:memory-graphs} shows the
results.



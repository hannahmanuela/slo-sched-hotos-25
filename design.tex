\section{Approach \& Design}\label{design}

Our approach addresses the variability of runtimes, which is undesirable for
latency sensitive functions, by using \priceclass{}es as a metric to tell \sys{} what
to prioritize and what not. We will show that this allows us to stabilize the
runtimes for the high \class{} functions. 

In fact, \sys{} uses \class{}es to supplant the current interface, which
requires developers choose an amount of memory per function, which is then tied
to a cpu power (a potentially fractional amount of vCPUs). \Priceclass{}es are
the only thing that developers need to give \sys{}, they pay for memory
separately, and only for what they use. The price for memory is the same across
all \class{}es. This serves the purpose of extending the serverless on-demand
structure to include memory, and moves away from the usual bin packing with
overprovisioning problem.

\Priceclass{}es are a metric that has a number of benefits over resource
reservations. One is that developers are more likely to have a good sense of
what \priceclass{} a function should have ahead of time, because they know in
what context the function will be used and how important it is that the function
run quickly. \Priceclass{}es also remain the same across different invocations,
whereas the resources needed can be heavily skewed in a web server environment,
where popularity distributions are often very
uneven~\cite{hermod,serverless-in-the-wild}. And finally, \class{}es more
directly align the interests of the developer with those of the provider by
communicating on the level of what the provider and developer actually care
about: money, and latency (as achieved by \class{}es in the system).

However, having \priceclass{}es also means that there are no absolute guarantees
about what developers are receiving when they put a price on a function they
want to run. In order to mitigate that somewhat and avoid the developer-side
uncertainty of bidding wars, \sys{} exposes a fixed set of \priceclass{}es. This
is similar to how AWS has different EC2 instance types, that are directly mapped
to prices. Rather than being a guarantee, the \priceclass{} is instead a metric
to express priority to \sys{}, which it can then use to enforce a favoring of
high class functions.

This also allows the provider to provision their datacenters on the level of the
amount of hardware they buy: by looking at the historical overall amount of high
\priceclass{} workload, they know how much hardware they at least need to buy in
order to be able to comfortably fit all of that load.


At the same time, \class{}es give \sys{} the information it needs to decide what
to schedule when. How exactly it does this is what we explore in \sys{}'s design
and evaluation.



\subsection{Interface}


Developers using \sys{} write function handlers and define triggers just like
they would for any existing serverless offering. In addition, each place where
they trigger the function, they assign that invocation to a \priceclass{}. For
instance, a simple web server might consist of a home page view that is assigned
a higher \priceclass{} and costs 2$\mu\cent$ per cpu second, a user profile page
view which is assigned a middle-high \class{} and cost 1.5$\mu\cent$ per cpu
second, and finally an image processing function that can be set to a low
\class{} which costs only 0.5$\mu\cent$ per cpu second.

\Class{}es are inherited across call chains: if a high \class{} function calls a
low \class{} function, that invocation with run with high \class{}. This is
important in order to avoid priority inversion.

To avoid unexpected costs in the case of for example a DOS attack or a bug,
developers also express a monthly budget that they are willing to pay.\ \sys{}
uses this budget as a guideline and throttles invocations or decreases quality
of service in the case that usage is not within reason given the expected
budget, though it does not guarantee that the budget will not be exceeded by
small amounts.



\subsection{\Sys{} Design}

\begin{figure}[t]
    \centering
      \includegraphics[width=9cm]{img/overview.png}
      \caption{ global scheduler shards queue and place functions (in orange), 
      on each machine a dispatcher thread keeps track of memory utilization 
      and if it's low writes itself to an idle list (in blue) }
    \label{fig:overview}
\end{figure}



\Sys{} has as its goal to enforce the \class{}es attached to functions, which
means it needs to prefer higher \class{} functions over lower ones, and preempt
the latter when necessary.
  

As shown in Figure~\ref{fig:overview}, \sys{} sits behind a load balancer, and
consists of: a \textit{distributed global scheduler}, which places new function
invocations and has attached an \textit{idle list}, a \textit{dispatcher},
which runs on each machine and communicates with the global scheduler shards,
and a \textit{machine scheduler}, which enforces \class{}es on the machines.


\textbf{Machine Scheduler.}
The machine scheduler is a preemptive priority scheduler: it preempts lower
\class{} functions to run higher \class{} ones. Being unfair and starving low
\class{} functions is desirable in \sys{}, since image processing functions
should not interrupt a page view request processing, but vice versa is expected.
Within \class{}es the machine scheduler is first come first served. This matches
Linux' `SCHED FIFO' scheduling~\cite{linux-sched}.


\textbf{Idle list.}
Each global scheduler shard has an idle list, which holds machines that have a
significant amount of memory available. In the shards idle list each machine's
entry is associated with the amount of free memory as well as the current amount
of functions on the machine. The idle list exists because datacenters are large:
polling a small number of machines has been shown to be very powerful, but
cannot find something that is a very rare occurrence~\cite{join-idle-queue}.
Having an idle list allows the machines that have actually idle resources, which
are expected to be rare in a high-utilization setting, to make themselves
visible to the global scheduler. The idle list also allows the global scheduler
to place high \class{} functions quickly, without incurring the latency
overheads of doing the polling to find available resources. This design is
inspired by join idle queue~\cite{join-idle-queue}, but defines idleness via
memory availability rather than empty queues.


\textbf{Dispatcher.}
The dispatcher is in charge of adding itself to a shard's idle list when memory
utilization is low. The dispatcher chooses which list to add itself to using
power-of-k-choices: it looks at k shards' idle lists and chooses the one with
the least other machines in it. If the machine is already on an idle list on
shard $i$, but the amount of available memory has changed significantly (either
by functions finishing and memory being freed or by memory utilization
increasing because of new functions or memory antagonists), the dispatcher will
update shard $i$'s idle list. These interactions from the dispatcher to free
lists are represented by the blue arrows in Figure~\ref{fig:overview}.

The dispatcher is also in charge of managing the machine's memory. When memory
pressure occurs, the dispatcher uses \textit{\class{}-based swapping} to move
low \class{} functions off the machine's memory. In this scenario, having
priority scheduling creates the opportunity that enables this to be realistic:
because the dispatcher knows that the lowest \class{} functions will not be run
until the high \class{} functions have all finished, it can swap its memory out
knowing it will not be needed soon. The dispatcher swaps the low \class{}
function back in when the memory pressure is gone and the function will be
run.\footnote{Whether Linux will simply do the right thing, or require heavy
hinting and guidance from the dispatcher, is an open question in \sys{}'s
design.}

Bounding the amount of swap space required without bounding the amount of memory
that functions can use is not possible. The goal of the dispatcher is to swap
when possible, and in the case that that is not enough it can resort to killing.
Providers can estimate the amount of swap space required by looking at memory
utilization, and since swap space is cheap~\cite{ssd-price} can provision it so
that killing is very rare.

\textbf{Global Scheduler Shards.}
Global scheduler shards store the functions waiting to be placed in a multi
queue, with one queue per \priceclass{}. Shards choose what function to place
next by looking at each function at the head of each queue, and comparing the
ratio of \class{} to amount of time spent in the queue. This ensures that high
\class{} functions don't have to wait as long as low \class{} functions to be
chosen next, but low \class{} functions will get placed if they have waited for
a while.

When placing the chosen function, the shard will first look in its idle list. If
the list is not empty, it will choose the machine with the smallest queue
length.

If there are no machines in the idle list, the shard switches over to
power-of-k-choices: it polls k machines, getting the amount of functions running
from each. The shard then places the new function on the machine with the
smallest number of currently running functions. 
